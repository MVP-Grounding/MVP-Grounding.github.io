\section{Conclusion}
In this paper, We find that current existing grounding models are highly sensitive to visual input variations. Even slight perturbations may flip the result between correct and incorrect, which indicates significant prediction instability and undermines model performance. To address this, we propose the training-free Multiple View Prediction (MVP) framework. MVP stabilizes predictions by cropping multiple views of screenshots, obtaining independent predictions, and aggregating them via spatial clustering. Extensive experiments on ScreenSpot-Pro, UI-Vision, and OS-World-G validate that MVP consistently and significantly boosts the performance of diverse grounding models, effectively unleashing their potential without any retraining.

% This paper identifies a critical limitation in GUI grounding models: significant prediction instability, where minor visual perturbations can flip results between correct and incorrect. To address this, we propose the training-free Multi-View Prediction (MVP) framework. MVP stabilizes predictions by generating multiple views of a screenshot, obtaining independent coordinate predictions, and aggregating them via spatial clustering. Extensive experiments on ScreenSpot-Pro, UI-Vision, and OS-World-G benchmarks validate that MVP consistently and significantly boosts the performance of diverse grounding models, effectively unleashing their latent potential without any retraining.
