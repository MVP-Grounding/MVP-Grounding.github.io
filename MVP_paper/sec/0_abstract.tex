\begin{abstract}
GUI grounding, which translates natural language instructions into precise pixel coordinates, is essential for developing practical GUI agents. However, we observe that existing grounding models exhibit significant \textbf{coordinate prediction instability}â€”minor visual perturbations (e.g., cropping a few pixels) can drastically alter predictions, flipping results between correct and incorrect. This instability severely undermines model performance, especially for samples with high-resolution and small UI elements. To address this issue, we propose Multi-View Prediction (MVP), a training-free framework that enhances grounding performance through multi-view inference. Our key insight is that while single-view predictions may be unstable, aggregating predictions from multiple carefully cropped views can effectively distinguish correct coordinates from outliers. MVP comprises two components: (1) \textbf{Attention-Guided View Proposal}, which derives diverse views guided by instruction-to-image attention scores, and (2) \textbf{Multi-Coordinates Clustering}, which ensembles predictions by selecting the centroid of the densest spatial cluster. Extensive experiments demonstrate MVP's effectiveness across various models and benchmarks. Notably, on ScreenSpot-Pro, MVP boosts UI-TARS-1.5-7B to 56.1\%, GTA1-7B to 61.7\%, Qwen3VL-8B-Instruct to 65.3\%, and Qwen3VL-32B-Instruct to 74.0\%. The code is available at \url{https://github.com/ZJUSCL/MVP}.
\end{abstract}