<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners -->
  <meta name="description" content="MVP: Multiple View Prediction Improves GUI Grounding - A training-free framework that enhances GUI grounding through multi-view inference">
  <meta property="og:title" content="MVP: Multiple View Prediction Improves GUI Grounding"/>
  <meta property="og:description" content="A training-free framework that enhances GUI grounding through multi-view inference, addressing coordinate prediction instability"/>
  <meta property="og:url" content="https://zjuscl.github.io/MVP-Grounding/"/>
  <meta property="og:image" content="static/images/output.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <meta name="twitter:title" content="MVP: Multiple View Prediction Improves GUI Grounding">
  <meta name="twitter:description" content="A training-free framework that enhances GUI grounding through multi-view inference">
  <meta name="twitter:image" content="static/images/output.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="GUI grounding, multi-view prediction, GUI agents, computer vision, vision-language models">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>MVP: Multiple View Prediction Improves GUI Grounding</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MVP: Multiple View Prediction Improves GUI Grounding</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><p>Yunzhu Zhang<sup>1</sup>,</p></span>
              <span class="author-block"><p>Zeyu Pan<sup>2</sup>,</p></span>
              <span class="author-block"><p>Zhengwen Zeng<sup>3</sup>,</p></span>
              <span class="author-block"><p>Shuheng Shen<sup>3</sup>,</p></span>
              <span class="author-block"><p>Changhua Meng<sup>3</sup>,</p></span>
              <span class="author-block"><p>Linchao Zhu<sup>1,†</sup></p></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Zhejiang University</span>
              <span class="author-block"><sup>2</sup>Hangzhou Dianzi University</span>
              <span class="author-block"><sup>3</sup>Ant Group</span>
              <span class="eql-cntrb"><small><br><sup>†</sup>Corresponding Author</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://github.com/ZJUSCL/MVP" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-github"></i></span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2512.08529" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-pdf"></i></span>
                    <span>Paper</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            We propose <strong>Multi-View Prediction (MVP)</strong>, a training-free framework that enhances models' GUI Grounding probability. 
            \<br>
            \<br>
            MVP crops different regions from original screenshot to get different prediction and then cluster them through k-means. The final result is decided by the centroid coordinate of the largest cluster. 
            \<br>
            \<br>
            MVP comprises two components: (1) <strong>Attention-Guided View Proposal</strong>, which derives diverse views guided by instruction-to-image attention scores, and (2) <strong>Multi-Coordinates Clustering</strong>, which ensembles predictions by selecting the centroid of the densest spatial cluster.
          </p>
        </div>

        <figure style="margin: 2rem auto; max-width: 100%;">
          <img src="./static/images/arch.pdf" alt="MVP framework overview" style="width: 100%; height: auto; display: block; margin: 0 auto;">
          <figcaption class="has-text-justified" style="margin-top: 1rem; font-size: 0.9rem; line-height: 1.5;">
            <p style="margin-bottom: 0.8rem;">Figure 1: <strong>Overview of MVP Framework.</strong> Given a high-resolution screenshot and user instruction, MVP first generates multiple cropped views using attention-guided view proposal. Each view is processed independently by the grounding model to produce coordinate predictions. These predictions are then aggregated through multi-coordinate clustering to produce the final robust prediction.</p>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Coordinate Prediction Instability</h2>
        <div class="content has-text-justified">
          <p>
            Existing GUI grounding models exhibit significant coordinate prediction instability. Minor visual perturbations, such as cropping a few pixels, can drastically alter predictions—flipping results between correct and incorrect. This instability is particularly severe for high-resolution screenshots with small UI elements. Our analysis reveals that single-view predictions are highly sensitive to input variations, making current models unreliable for practical GUI agent applications.
          </p>
        </div>

        <figure style="margin: 2rem auto; max-width: 100%;">
          <img src="./static/images/resolution_analysis.png" alt="Resolution analysis" style="width: 70%; height: auto; display: block; margin: 0 auto;">
          <figcaption class="has-text-justified" style="margin-top: 1rem; font-size: 0.9rem; line-height: 1.5;">
            <p style="margin-bottom: 0.8rem;">Figure 2: <strong>Impact of Resolution on Grounding Performance.</strong> Higher resolutions lead to more unstable predictions due to the increased number of small UI elements and context complexity.</p>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Attention-Guided View Proposal</h2>
        <div class="content has-text-justified">
          <p>
            To generate informative views, we leverage the attention maps from vision-language models. The attention scores between the instruction and image regions guide us to identify and crop relevant areas. This attention-guided approach ensures that multiple views focus on different parts of the screen while maintaining relevance to the grounding task, enabling more robust coordinate predictions.
          </p>
        </div>

        <figure style="margin: 2rem auto; max-width: 100%;">
          <img src="./static/images/arch.pdf" alt="Architecture diagram" style="width: 90%; height: auto; display: block; margin: 0 auto;">
          <figcaption class="has-text-justified" style="margin-top: 1rem; font-size: 0.9rem; line-height: 1.5;">
            <p style="margin-bottom: 0.8rem;">Figure 3: <strong>Architecture of Attention-Guided View Proposal.</strong> The module uses cross-modal attention to identify regions most relevant to the instruction and generates diverse cropped views for multi-view inference.</p>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Multi-Coordinates Clustering</h2>
        <div class="content has-text-justified">
          <p>
            After obtaining predictions from multiple views, we aggregate them through spatial clustering. By clustering all predicted coordinates, we can identify the densest region where consistent predictions converge. The centroid of this cluster is selected as the final prediction, effectively filtering out outliers and unstable predictions from individual views.
          </p>
        </div>

        <figure style="margin: 2rem auto; max-width: 100%;">
          <img src="./static/images/prediction_analysis_pie_chart.png" alt="Prediction analysis" style="width: 70%; height: auto; display: block; margin: 0 auto;">
          <figcaption class="has-text-justified" style="margin-top: 1rem; font-size: 0.9rem; line-height: 1.5;">
            <p style="margin-bottom: 0.8rem;">Figure 4: <strong>Distribution Analysis of Coordinate Predictions.</strong> The clustering process identifies the densest region of consistent predictions while filtering out outliers.</p>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
        <div class="content has-text-justified">
          <p>
            MVP significantly improves performance across various foundation models. On the ScreenSpot-Pro benchmark, our method achieves consistent improvements across all tested models:
            <br><br>
            <strong>UI-TARS-1.5-7B:</strong> 49.3% → 56.1% (+6.8%)<br>
            <strong>GTA1-7B:</strong> 54.9% → 61.7% (+6.8%)<br>
            <strong>Qwen3VL-8B-Instruct:</strong> 59.6% → 65.3% (+5.7%)<br>
            <strong>Qwen3VL-32B-Instruct:</strong> 68.3% → 74.0% (+5.7%)
          </p>
        </div>

        <figure style="margin: 2rem auto; max-width: 100%;">
          <img src="./static/images/multi_model_radar_comparison_grouped.png" alt="Multi-model comparison" style="width: 90%; height: auto; display: block; margin: 0 auto;">
          <figcaption class="has-text-justified" style="margin-top: 1rem; font-size: 0.9rem; line-height: 1.5;">
            <p style="margin-bottom: 0.8rem;">Figure 5: <strong>Performance Comparison Across Different Foundation Models.</strong> MVP consistently improves grounding performance across various model architectures without requiring additional training.</p>
          </figcaption>
        </figure>

        <figure style="margin: 2rem auto; max-width: 100%;">
          <img src="./static/images/sspro_avg_score_vs_view_number.png" alt="View number analysis" style="width: 80%; height: auto; display: block; margin: 0 auto;">
          <figcaption class="has-text-justified" style="margin-top: 1rem; font-size: 0.9rem; line-height: 1.5;">
            <p style="margin-bottom: 0.8rem;">Figure 6: <strong>Average Accuracy vs Number of Views.</strong> Using 3-5 views achieves the best balance between performance and computational cost on ScreenSpot-Pro benchmark.</p>
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Qualitative Results</h2>
        <div class="content has-text-justified">
          <p>
            The figure visualizes qualitative examples of MVP improving GUI grounding. For each example, we show the original screenshot with the instruction, attention heatmap, cropped views, and final predictions. Our method successfully handles various challenges including small UI elements, complex layouts, and ambiguous instructions.
          </p>
        </div>

        <figure style="margin: 2rem auto; max-width: 100%;">
          <img src="./static/images/example1.pdf" alt="Qualitative examples" style="width: 100%; height: auto; display: block; margin: 0 auto;">
          <figcaption class="has-text-justified" style="margin-top: 1rem; font-size: 0.9rem; line-height: 1.5;">
            <p style="margin-bottom: 0.8rem;">Figure 7: <strong>Qualitative Examples.</strong> Demonstrating MVP's effectiveness on challenging GUI grounding cases with small elements and complex layouts.</p>
          </figcaption>
        </figure>

        <figure style="margin: 2rem auto; max-width: 100%;">
          <img src="./static/images/example2.pdf" alt="More examples" style="width: 100%; height: auto; display: block; margin: 0 auto;">
        </figure>
        <figure style="margin: 2rem auto; max-width: 100%;">
          <img src="./static/images/example3.pdf" alt="Additional examples" style="width: 100%; height: auto; display: block; margin: 0 auto;">
        </figure>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zhang2025mvp,
  title={MVP: Multiple View Prediction Improves GUI Grounding},
  author={Zhang, Yunzhu and Pan, Zeyu and Zeng, Zhengwen and Shen, Shuheng and Meng, Changhua and Zhu, Linchao},
  journal={arXiv preprint arXiv:2512.08529},
  year={2025},
  url={https://github.com/ZJUSCL/MVP}
}</code></pre>
  </div>
</section>
<!--End BibTex citation -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
